{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:08<00:00, 30.2MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "#load pretrained model/tokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [['testing', 'how', 'a', 'transform', '##er', 'model', 'works', 'with', 'hugging', 'face', '.'], ['testing', 'how', 'a', 'transform', '##er', 'model', 'works', 'with', 'hugging', 'face', '.']]\n",
      "IDs: tensor([[ 5604,  2129,  1037, 10938,  2121,  2944,  2573,  2007, 17662,  2227,\n",
      "          1012],\n",
      "        [ 5604,  2129,  1037, 10938,  2121,  2944,  2573,  2007, 17662,  2227,\n",
      "          1012]])\n",
      "Output: tensor([[ 1.7090, -1.3880],\n",
      "        [ 1.7090, -1.3880]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sequence1 = \"Testing how a transformer model works with hugging face.\"\n",
    "seq_batch = [sequence1, sequence1]\n",
    "tokens = [tokenizer.tokenize(sequence) for sequence in seq_batch]\n",
    "ids = [tokenizer.convert_tokens_to_ids(token_list) for token_list in tokens]\n",
    "input = torch.tensor(ids)\n",
    "model_out = model(input, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", input)\n",
    "print(\"Output:\", model_out.logits)\n",
    "print(model_out.logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_inputs: {'input_ids': tensor([[  101,  5604,  2129, 19081,  2147,  1012,   102,     0,     0,     0],\n",
      "        [  101,  2023,  6251,  3310,  2044,  1996,  2034,  2028,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "logits: tensor([[-0.1780,  0.3090],\n",
      "        [ 1.9908, -1.6774]], grad_fn=<AddmmBackward0>)\n",
      "classification: tensor([[0.3806, 0.6194],\n",
      "        [0.9751, 0.0249]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sequences = ['Testing how transformers work.', 'This sentence comes after the first one.']\n",
    "model_inputs = tokenizer(sequences, padding='longest', truncation=True, return_tensors=\"pt\")\n",
    "model_out = model(**model_inputs)\n",
    "print('model_inputs:', model_inputs)\n",
    "print('logits:',model_out.logits)\n",
    "scores = torch.nn.functional.softmax(model_out.logits, dim=1)\n",
    "print('classification:', scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple pipeline version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The first thing to learn about dodgeball is how simple it is. A block is either a vertical or a horizontal block with a horizontal, vertical or horizontal defender. Your opponent should be well past this mark, with the ability to dodge this pass,'},\n",
       " {'generated_text': 'The first thing to learn about dodgeball is to look at the angles and how long the ball must travel before you can hit the ground. It is important to know which angles to use.\\n\\nIn this page we teach you how to set it'},\n",
       " {'generated_text': \"The first thing to learn about dodgeball is that you'll have to do some reading before you can spot the perfect spot to use your feet. In order to find a perfect spot, you usually want to take breaks from the pool and the practice game\"},\n",
       " {'generated_text': \"The first thing to learn about dodgeball is you can do a simple block of time, which, frankly, doesn't look anything like the skill you learn in school. Take the long approach to dodgeball. This is something every kid knows, but\"},\n",
       " {'generated_text': \"The first thing to learn about dodgeball is getting a grasp of the concepts of defense. Defense is a basic act used to block shots – it's the easiest method to dodge to put a shot in the net, to put down a shot, or\"}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"The first thing to learn about dodgeball is\", max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complete version. Experimenting with different text generation strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[ 6307, 40735,   468, 33922,   262,   869,   319,   262,  3072,   351,\n",
      "           978,  4476,   505,   546,   465,  1138,  1313,   462,    13]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "\"I'm not going to talk to him about it,\" Malone said. \"I'm not going to talk to him about it. I'm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Search: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "\"I don't want to talk about it,\" Malone said. \"I don't want to talk about it. I don't want to talk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling: Post Malone has postponed the call on the phone with Al Capone about his metronome. Al's spokesman said the actor declined to talk to Malone about the situation.\n",
      "\n",
      "ESPN Radio has been conducting an investigation and interviewed Malone about the story,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam+Sampling: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "\"I don't know what he's going to do,\" Malone said. \"I don't know what he's going to do.\"\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContrastiveSearch: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "\"I'm going to have to get back to New York,\" Malone said. \"I'm going to have to get back to New York.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiverseBeam: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "\"I'm not going to talk to him about it,\" Malone said. \"I'm not going to talk to him about it. I'm\n",
      "SamplingHiTemp: Post Malone has postponed the call on the phone with Al Capone about his metronome. After all, who was the first one ever born on the outside of the dome? That would certainly mean that the Al-Wasl, the \"Old\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Post Malone has postponed the call on the phone with Al Capone about his metronome.\"\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "print('Input:', input)\n",
    "generated = model.generate(input, max_length=50, do_sample=False, num_beams=1, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('Greedy:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, do_sample=False, num_beams=5, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('Beam Search:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, do_sample=True, num_beams=1, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('Sampling:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, do_sample=True, num_beams=5, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('Beam+Sampling:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, penalty_alpha=0.6, top_k=4, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('ContrastiveSearch:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, num_beam_groups=5, num_beams=5, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('DiverseBeam:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, do_sample=True, temperature=1.5, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('SamplingHiTemp:', tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling and SamplingHiTemp seem to give the best results. The other variants have a large amount of repetition.\n",
    "\n",
    "Retrying the experiment with the larger GPT-2 model to see if the trend is the same or different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 666/666 [00:00<?, ?B/s] \n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 43.4MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 65.2MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 19.6MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 3.25G/3.25G [01:54<00:00, 28.3MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 124kB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[ 6307, 40735,   468, 33922,   262,   869,   319,   262,  3072,   351,\n",
      "           978,  4476,   505,   546,   465,  1138,  1313,   462,    13]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "\"I'm not going to call him,\" Malone said. \"I'm not going to call him. I'm not going to call him.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Search: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling: Post Malone has postponed the call on the phone with Al Capone about his metronome. The legendary music mogul has never heard from the New York City rap star since a few days after a New York Newsday article on the New York Post's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam+Sampling: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "\"I don't want to get into it,\" Malone said. \"I don't want to get into it. I don't want to get\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContrastiveSearch: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "Capone is in the middle of a prison sentence for racketeering in New York, according to TMZ. The rap legend's rep says he's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiverseBeam: Post Malone has postponed the call on the phone with Al Capone about his metronome.\n",
      "\n",
      "\"I'm not going to call him,\" Malone said. \"I'm not going to call him. I'm not going to call him.\n",
      "SamplingHiTemp: Post Malone has postponed the call on the phone with Al Capone about his metronome. A few hours later, I've called to book a taxi for someone who can deliver our gear. Capone knows. Or at least, he did.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir='E:\\\\cache')\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir='E:\\\\cache')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "print('Input:', input)\n",
    "generated = model.generate(input, max_length=50, do_sample=False, num_beams=1, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('Greedy:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, do_sample=False, num_beams=5, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('Beam Search:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, do_sample=True, num_beams=1, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('Sampling:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, do_sample=True, num_beams=5, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('Beam+Sampling:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, penalty_alpha=0.6, top_k=4, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('ContrastiveSearch:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, num_beam_groups=5, num_beams=5, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('DiverseBeam:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
    "generated = model.generate(input, max_length=50, do_sample=True, temperature=1.5, attention_mask=torch.ones(input.shape, dtype=torch.long))\n",
    "print('SamplingHiTemp:', tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling provides the best result in this case. A lot of the issues from the smaller GPT-2 model are present in the larger model. This time, ContrastiveSearch provides more contextual information that is valuable, and SamplingHiTemp provides a weird output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
